#!/bin/sh

# Parse date argument
date=20170228

if [[ -n $1 ]]
then
    date=$1
fi

date_length=${#date}
if [[ $date_length != 8 ]]
then
    echo 'Invalid date. Example: 20170228'
    exit
fi

echo 'Aggregating for date: '$date

# Get current script's absolute path
scriptpath="$( cd "$(dirname "$0")" ; pwd -P )"

# Results will be put here
location=/cms/users/$USER/campaigns
hdir=hdfs://$location

# Remove previous data first
hadoop fs -rm -r $location

PYTHONPATH=$scriptpath/../../python $scriptpath/../../../bin/run_spark /reports/aggregate_campaigns.py --fout=$hdir --yarn --verbose --date=$date

hadoop fs -test -e $hdir
exists=$?

# Download results and recreate csv files only if results exist in hdfs
if [[ $exists -eq 0 ]]
then
    # Delete previously downloaded directory and download new one
    basename $hdir | xargs rm -rf
    hadoop fs -get $hdir .

    # Extract PhEDEx header
    head -1 campaigns/phedex/part-00000 > campaigns_phedex_df.csv

    # Concatenate all PhEDEx parts except header
    header=`cat campaigns_phedex_df.csv`
    cat campaigns/phedex/part* | grep -v $header >> campaigns_phedex_df.csv

    # Extract DBS header
    head -1 campaigns/dbs/part-00000 > campaigns_dbs_df.csv

    # Concatenate all DBS parts except header
    header=`cat campaigns_dbs_df.csv`
    cat campaigns/dbs/part* | grep -v $header >> campaigns_dbs_df.csv

    # Extract site - campaign count header
    head -1 campaigns/site_campaign_count/part-00000 > site_campaign_count_df.csv

    # Concatenate all site - campaign count parts except header
    header=`cat site_campaign_count_df.csv`
    cat campaigns/site_campaign_count/part* | grep -v $header >> site_campaign_count_df.csv
fi
