{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If ran from SWAN, connect to Analytix Spark Cluster\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    LongType, \n",
    "    StringType, \n",
    "    StructField,\n",
    "    DoubleType\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    concat,\n",
    "    when,\n",
    "    mean,\n",
    "    sum as _sum,\n",
    "    countDistinct,\n",
    "    first\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "base_folder = \"../../www/\"\n",
    "os.makedirs(base_folder, exist_ok = True)\n",
    "# Set up the time span for which we want to look at datasets\n",
    "days = 30\n",
    "end_date = datetime.combine(date.today() - timedelta(days=1), datetime.min.time())\n",
    "start_date = end_date - timedelta(days=days)\n",
    "cms_type = 'production' # 'analysis' 'test'\n",
    "min_eff = 5\n",
    "max_eff = 70\n",
    "group_type_map = {'production':['Workflow','WMAgent_RequestName'],\n",
    "                  'analysis': ['Workflow'],\n",
    "                  'test': ['Wokflow']\n",
    "                 }\n",
    "group_by_col = group_type_map[cms_type]\n",
    "# Set up pandas\n",
    "pd.set_option('display.max_colwidth', -1) # never cut long columns\n",
    "pd.options.display.float_format = '{:,.2f}'.format # only 2 decimals\n",
    "def format_df(df):\n",
    "    df = df.rename(columns={\"wf_cpueff\": \"CPU_eff\",\n",
    "                            \"mean_cpueff\" : \"mean_CPU_eff\",\n",
    "                            \"wf_cpus\": \"CPUs\",\n",
    "                            \"wf_cputimehr\" : \"CPU_time_hr\",\n",
    "                            \"wf_wallclockhr\" : \"Wall_time_hr\"})\n",
    "\n",
    "    df['CPU_eff'] = df['CPU_eff'].map('{:,.1f}%'.format)\n",
    "    df['mean_CPU_eff'] = df['mean_CPU_eff'].map('{:,.1f}%'.format)\n",
    "    df['CPUs'] = df['CPUs'].map(int)\n",
    "    df['CPU_time_hr'] = df['CPU_time_hr'].map(int)\n",
    "    df['Wall_time_hr'] = df['Wall_time_hr'].map(int)\n",
    "    return df\n",
    "\n",
    "def get_candidate_files(start_date, end_date, spark, base=\"/project/monitoring/archive/condor/raw/metric/\"):\n",
    "    \"\"\"\n",
    "    Returns a list of hdfs folders that can contain data for the given dates.\n",
    "    \"\"\"\n",
    "    st_date = start_date - timedelta(days=1)\n",
    "    ed_date = end_date + timedelta(days=1)\n",
    "    days = (ed_date - st_date).days\n",
    "    pre_candidate_files = [\n",
    "        \"{base}/{day}{{,.tmp}}\".format(\n",
    "            base=base, day=(st_date + timedelta(days=i)).strftime(\"%Y/%m/%d\")\n",
    "        )\n",
    "        for i in range(0, days)\n",
    "    ]\n",
    "    sc = spark.sparkContext\n",
    "    # The candidate files are the folders to the specific dates,\n",
    "    # but if we are looking at recent days the compaction procedure could\n",
    "    # have not run yet so we will considerate also the .tmp folders.\n",
    "    candidate_files = [\n",
    "        \"/project/monitoring/archive/condor/raw/metric/{}{{,.tmp}}\".format(\n",
    "            (st_date + timedelta(days=i)).strftime(\"%Y/%m/%d\")\n",
    "        )\n",
    "        for i in range(0, days)\n",
    "    ]\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    URI = sc._gateway.jvm.java.net.URI\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    fs = FileSystem.get(URI(\"hdfs:///\"), sc._jsc.hadoopConfiguration())\n",
    "    candidate_files = [url for url in candidate_files if fs.globStatus(Path(url))]\n",
    "    return candidate_files\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('data', StructType([\n",
    "        StructField(\"GlobalJobId\", StringType(), nullable=False),\n",
    "        StructField(\"Workflow\", StringType(), nullable=False),\n",
    "        StructField(\"WMAgent_RequestName\", StringType(), nullable=True),\n",
    "        StructField(\"ScheddName\", StringType(), nullable=True),\n",
    "        StructField(\"WMAgent_JobID\", StringType(), nullable=True),\n",
    "        StructField(\"RecordTime\", LongType(), nullable=False),\n",
    "        StructField(\"JobFailed\", LongType(), nullable=False),\n",
    "        StructField(\"Status\", StringType(), nullable=True),\n",
    "        StructField(\"Site\", StringType(), nullable=True),\n",
    "        StructField(\"Type\", StringType(), nullable=True),\n",
    "        StructField(\"WallClockHr\", DoubleType(), nullable=False),\n",
    "        StructField(\"CpuTimeHr\", DoubleType(), nullable=True),\n",
    "        StructField(\"RequestCpus\", DoubleType(), nullable=True),\n",
    "        StructField(\"CpuEff\", DoubleType(), nullable=True),\n",
    "    ])),\n",
    "])\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "folder = \"/project/monitoring/archive/condor/raw/metric/\"\n",
    "raw_df = (spark.read.option(\"basePath\",folder)\n",
    "          .json(get_candidate_files(start_date, end_date, spark, base=folder),schema=schema)\n",
    "          .select(\"data.*\")\n",
    "          .filter(f\"\"\"Status='Completed'\n",
    "          AND JobFailed=0\n",
    "          AND RecordTime >= {start_date.timestamp()*1000}\n",
    "          AND RecordTime < {end_date.timestamp()*1000}\n",
    "          AND Type =  '{cms_type}'\n",
    "          \"\"\")\n",
    "          .drop_duplicates(['GlobalJobId'])\n",
    "        )\n",
    "\n",
    "raw_df = (raw_df.withColumn(\"RequestCpus\",\n",
    "                            when(col(\"RequestCpus\").isNotNull(),\n",
    "                                 col(\"RequestCpus\"))\n",
    "                            .otherwise(lit(1)))\n",
    "          .withColumn(\"CoreTime\", col(\"WallClockHr\")*col(\"RequestCpus\"))\n",
    "         ).cache()\n",
    "\n",
    "\n",
    "grouped_wf = (raw_df.groupby(*group_by_col, \"Type\")\n",
    "                    .agg(mean(\"CpuEff\").alias(\"mean_cpueff\"), \n",
    "                         (100*_sum(\"CpuTimeHr\")/_sum(\"CoreTime\")).alias(\"wf_cpueff\"),\n",
    "                         _sum(\"RequestCpus\").alias(\"wf_cpus\"),\n",
    "                         _sum(\"CpuTimeHr\").alias(\"wf_cputimehr\"),\n",
    "                         _sum(\"WallClockHr\").alias(\"wf_wallclockhr\")\n",
    "                        )\n",
    "                    \n",
    "             )\n",
    "grouped_site_wf = (raw_df.groupby(*group_by_col, \"Site\")\n",
    "                    .agg(mean(\"CpuEff\").alias(\"mean_cpueff\"), \n",
    "                         (100*_sum(\"CpuTimeHr\")/_sum(\"CoreTime\")).alias(\"wf_site_cpueff\"),\n",
    "                         _sum(\"RequestCpus\").alias(\"wf_cpus\"),\n",
    "                         _sum(\"CpuTimeHr\").alias(\"wf_site_cputimehr\"),\n",
    "                         _sum(\"WallClockHr\").alias(\"wf_site_wallclockhr\"),\n",
    "                         first(\"ScheddName\").alias(\"schedd\"),\n",
    "                         first(\"WMAgent_JobID\").alias(\"wmagent_jobid\"),\n",
    "                         \n",
    "                        )\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here the first heavy query to spark\n",
    "#cms_types = grouped_wf.select('Type').distinct().toPandas().Type.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now comes the light part, if possible re-execute from here on to avoid heavy calculations\n",
    "\n",
    "\n",
    "select_expr = f\"\"\"wf_cpueff BETWEEN {min_eff} AND {max_eff} AND wf_wallclockhr BETWEEN 100 AND 100000000000\n",
    "\"\"\"\n",
    "\n",
    "print(select_expr)\n",
    "selected_df = (grouped_wf.where(select_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the second, light query to spark\n",
    "selected_pd = selected_df.toPandas()\n",
    "selected_pd.to_csv(f\"workflows_cpueff{min_eff}-{max_eff}_{'-'.join(select_expr)}.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_column = selected_pd['Workflow'].copy()\n",
    "filter_column = workflow_column if group_by_col[-1] == 'Workflow' else selected_pd[group_by_col[-1]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_wf = filter_column.name == 'Workflow'\n",
    "selected_pd['Workflow'] = f'<a class=\"wfname{\" selname\" if is_wf else \"\"}\">'+workflow_column+'</a><br><a target=\"_blank\" href=\"https://cms-pdmv.cern.ch/mcm/requests?prepid='+workflow_column+'\">McM</a> '\\\n",
    "                            '<a target=\"_blank\" href=\"https://dmytro.web.cern.ch/dmytro/cmsprodmon/workflows.php?prep_id=task_'+workflow_column+'\">PMon</a>'\n",
    "if not is_wf:\n",
    "    _fc = '<a class=\"selname\">'+filter_column+'</a>'\n",
    "    if filter_column.name == \"WMAgent_RequestName\":\n",
    "        _fc += '<br/><a href=\"https://cms-unified.web.cern.ch/cms-unified/logmapping/'+filter_column+'/\">logs</a>'\n",
    "    selected_pd[filter_column.name] = _fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = format_df(selected_pd).to_html(escape=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleanup of the default dump\n",
    "html = html.replace('table border=\"1\" class=\"dataframe\"','table id=\"dataframe\" class=\"display compact\" style=\"width:100%;\"' )\n",
    "html = html.replace('style=\"text-align: right;\"','')\n",
    "\n",
    "html_header = f'''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<link rel=\"stylesheet\" href=\"https://cdn.datatables.net/1.10.20/css/jquery.dataTables.min.css\">\n",
    "<style>\n",
    "table td {{\n",
    "word-break: break-all;\n",
    "}}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<h2>Dump of CMSSW Workflows and Their efficiencies from {start_date.strftime(\"%A %d. %B %Y\")} to {end_date.strftime(\"%A %d. %B %Y\")}</h2>\n",
    " <ul>\n",
    "  <li><b>mean_CPU_eff</b>: avg_cpu_time / (avg_wall_clock_time * n_cores)</li>\n",
    "  <li><b>CPU_eff</b>: avg( cpu_time / (wall_clock_time * n_cores) )</li>\n",
    "</ul>\n",
    "<div class=\"container\" style=\"display:block; width:100%\">\n",
    "'''\n",
    "html_footer = '''\n",
    "</div>\n",
    "<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js\"></script>\n",
    "<script type=\"text/javascript\" src=\"https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js\"></script>\n",
    "<script>\n",
    "    $(document).ready(function () {\n",
    "    function toggleDetails(){\n",
    "            var tr = $(this).closest(\"tr\");\n",
    "            sel_name = $(tr).find(\"td a.selname\").text()\n",
    "            wf_name = $(tr).find(\"td a.wfname\").text()\n",
    "            d_class=\"details-show\"\n",
    "            row = dt.row(tr)\n",
    "            if(!row.child.isShown())\n",
    "            { \n",
    "                console.log(wf_name)\n",
    "                $(tr).addClass(d_class)\n",
    "                row.child(\"<div id='details_\"+sel_name+\"'>loading</div>\").show()\n",
    "                folder = \"wfbysite\"'''+('' if len(group_by_col) ==1 else \"+'/'+wf_name\")+'''\n",
    "                $.get(folder+\"/CPU_Efficiency_bySite_\"+sel_name+\".html\", function (response){ \n",
    "                    var html = response;\n",
    "                    $(\"#details_\"+sel_name).html(html);\n",
    "                });\n",
    "                \n",
    "            }else{\n",
    "                $(tr).removeClass(d_class)\n",
    "                row.child.hide()\n",
    "            }\n",
    "            \n",
    "        }\n",
    "        $('table#dataframe thead tr').append('<th>site details</th>');\n",
    "        $('table#dataframe tbody tr').append('<td><button class=\"btn-details\">+</button></td>');\n",
    "        var dt = $('#dataframe').DataTable( {\n",
    "        \"order\": [[ 4, \"asc\" ]],\n",
    "        \"scrollX\": false,\n",
    "\n",
    "        });\n",
    "        $('table#dataframe tbody tr').on('click','td button.btn-details',toggleDetails)\n",
    "        dt.on('draw', function(){\n",
    "        $('table#dataframe tbody tr').off('click').on('click','td button.btn-details',toggleDetails)\n",
    "        })\n",
    "    });\n",
    "</script></body></html>'''\n",
    "html = html_header + html + html_footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{base_folder or '.'}/CPU_Efficiency_Table.html\",\"w\") as ofile:\n",
    "    ofile.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are only interested on the selected workflows.\n",
    "site_wf = grouped_site_wf.where(\n",
    "    col(filter_column.name).isin(filter_column.to_list())\n",
    ").toPandas()\n",
    "if cms_type == 'production':\n",
    "    site_wf[\"log\"] = \"<a href='https://cms-unified.web.cern.ch/cms-unified/logmapping/\"+site_wf[\"WMAgent_RequestName\"]+\"/\"+site_wf[\"schedd\"]+\"_\"+site_wf[\"wmagent_jobid\"]+\".tar.gz'>logs</a>\"\n",
    "    site_wf.drop(columns=\"schedd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_wf = site_wf.set_index([*group_by_col, \"Site\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create one file per worflow, so we don't have a big file collapsing the browser. \n",
    "_folder = f\"{base_folder or '.'}/wfbysite\"\n",
    "os.makedirs(_folder, exist_ok=True)\n",
    "num_levels = len(group_by_col)\n",
    "for workflow, df in site_wf.groupby(filter_column.name):\n",
    "    sublevels = \"\"\n",
    "    if num_levels>1:\n",
    "        df_ni = df.reset_index()\n",
    "        sublevels = \"/\".join(df_ni[group_by_col[0:-1]].drop_duplicates().values[0].tolist())+\"/\"\n",
    "        os.makedirs(f\"{_folder}/{sublevels}\", exist_ok=True)\n",
    "    df.droplevel(list(range(num_levels))).to_html(f\"{_folder}/{sublevels}CPU_Efficiency_bySite_{workflow}.html\", escape=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.executor.memory",
     "value": "10g"
    },
    {
     "name": "spark.executor.cores",
     "value": "4"
    },
    {
     "name": "spark.executor.instances",
     "value": "64"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
