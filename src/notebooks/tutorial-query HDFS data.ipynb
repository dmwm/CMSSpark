{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query HDFS Data\n",
    "In this tutorial you will query the HDFS data you ingested using the MonIT flow. \n",
    "## Prerequisites \n",
    "  - A swan environment:\n",
    "      - If you haven't used swan or cernbox before login first in [cernbox](http://cernbox.cern.ch/) to have you eos folder created.\n",
    "      - If you haven't used HDFS at cern before, you will need to create a [SNOW ticket to the Hadoop service element](https://hadoop-user-guide.web.cern.ch/hadoop-user-guide/getstart/access.html) to have your account authorized.\n",
    "        Please specify that you will use the Analytix cluster to access CMS data. \n",
    "      - go to https://swan.cern.ch, login and create a new session with the following configuration:\n",
    "          - Software stack `96 Python 3`\n",
    "          - Platform `CentOS 7 (gcc8)`\n",
    "          - Number of cores 4\n",
    "          - Memory 8 GB\n",
    "          - Spark cluster `Analytix`\n",
    "  - This notebook.\n",
    "      - In swan, create a new project (using the + button). Enter to the project folder. \n",
    "      - Download this notebook and import it to the new project using the upload file button.\n",
    "\n",
    "# Before start running the notebook      \n",
    "\n",
    "Swan will take care of the creation of the spark session/spark context variables. In order to do so you need to start the spark session using the star icon.\n",
    "![]( data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADwAAAA8CAYAAAA6/NlyAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH4QsPERAZDTIwFgAABYBJREFUaN7tmVtMHGUYhp8FdhcWOVNrObSAUmptE4209Www2pp44SHGXjUmJtp4uLFW44Ve28ZDook3xmhijDcajfUYNWls1VQrRVFbKrVYC00VymEBl3LY9YJ3kj+/CyzDzi6u8yYT2Nn9Z+b9v9P7fQM+fPjw4cOHDx8+/rsoAELL5WHyPb7+JmAHcDPwJ9Cfy5ZtAt4FhoBR4EugOJcJ3wKcABI6poFXgbJsPlSeh9e+GCi1wudO4EGgMNcI5wElSZJVBfAIsDXXCBcCdUDYOh8ALgIeA67IJcLFQA0Q1OeE5dpbZOnGXCFcDZTLogAx4BQwoc9hxfO9LpNYUPV92dThtcAdQK1I9wHPKok5Vi0C1gCngeNAfAHDFAGVQAtwPXClnn9ggbX/UkFe4EKgyrDwIPCprNxgkG4AdgJnVKdNhBUaK7SB6/R3rSpANfAdsAtot8Imo4QDepgLjHODwCTwCfAy8IQ2JQBco1J1DvhN55tkyWaDYK2uaYZhqyzdqetnhXBYhIPGuSHFcRx4U1bbpd8EgduAGQmVJpGsVxkLG55iYxwYXoxLe4EyYI/kpKOyXrHyRRPwjvF9QhsSBc5b5+c73gJWZztL5ym5ODU4DnTLgg56gBeAg1btLpmjsxqUNDVxDHgN6F1s65ZuVFo1eFLxaSKhhLNXv7/M+n4UOAn8DHRIrOzUhgD8DbwNHF6sO6eDcIEeZBVwKdBmqahRoCvJumngC+A5JbEyWb4TOCIy3arne6wkeAR4HxjJVEPvKKk24FHgDeBbuVfMirMRWXE+NAPXauMCRpLKBx6w8sEw8HgS2ZpWCxdLNNQAG2XBDdLLtQv0uZ8rEc2Hbh3JBMz9lnXbgX0pXHNJImI38LFi6twCmXNaEjIK/KCS4yY5BuXK5rXPAvd47cL3KUvORXBGBEeUaD4DXgQelqhw2/veIILOfSaBl5YqhwtS+N5u5M1M64iFA8CvUkpdwO9Go+C2vdwNrDTu9aMmJjNeWjcAbAO+n8OyMcXTdfOoITe4SSoqobIT1QYsGam4R4+OYu14kbEZeZJ/G1Qro5KRS5V6V2kqUqT7jAPvycqeE07IVb8C/lA5qJLbBZRBG9XUX67PQ4p7txgH1quBcJ5zUsps3GvCpoDolHv3inij1TQ0MDuL3qjNOO0ylod1v9slNfNVozuUMzJC2NHF/SL+jbJog9zaQQS4RG4+qvrqpmaeUf6o1+eQrndY0jIjhB1MiXi7NPGEREmpEd9Vcr9DcvHFYkbX26RYDsrKx+YQKRkZ8UzJbQ/K6qVq15xydxb4yGU8J7SuxYjlCmAM+Npt2UvHTCshlz0h8XG1rIu8YJ/mTsnayBo1HObAL261hSGVqbDW5Mtr+rzU0qkSH9Nhtop1EiVx675bpMY2y1Mc0XJSs6/j2rAOrW/V2tUqWV1uuqV098NjqsVm5i6XZeLGua3AUyLrYJ10tzO26QOOSgOMqiyFtIm3St0dSHV45xUamH1j6KixAeBuI3SKgO3qZ1Md48S0kTPWuaetLiorFj5vCYMSkUzo/x3Ak0apcSYiI7J8RJsTsHR1Mq29WcpvLJuEp+XSjvuFmB22h4CHpIerrSz/oWTjCo16GkWkUlk/ModOX69630MWB/Ex4C9ZOqQHaVZysslGNZfaa9TViMKiVhm8RRPOOq2t12+c9WOLjeF0E55QKZkyzrUBd1nx1g+8DjyvDcIYzh3VgdaUyfqrlNiaJUI+MHJB1hDQSGZgniTUq4Sz0qVuqEhhRpZRbBepZGR/0YaUk0O4UcRssqfk2oXkGFo1vDPJdkseBslBrGH2vZEjFPar3OQth4fz4oV4VDEcV+v4DPBTtrNpJhDB5dsBHz58+PDxf8U/BdyLhkKiXK8AAAAASUVORK5CYII=)\n",
    "For this notebook you can start the session using the default configuration. \n",
    "Before clicking connect take some time to explore this interface, some useful variables you could set here are:\n",
    "  - spark.executor.memory\n",
    "  - spark.driver.memory\n",
    "  - spark.executor.instances\n",
    "  - spark.executor.cores\n",
    "  - spark.sql.shuffle.partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are connected to the spark session two variables are created for you:\n",
    "    - `spark`: The spark session\n",
    "    - `sc`: The spark context\n",
    "Run the next cell to see more information about them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(spark)\n",
    "display(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as you are now connected to a hadoop cluster, you can use the Hadoop (elephant) icon to browse the data in HDFS. Use it to see the available dates in `/project/monitoring/archive/condor/raw/metric/`\n",
    "![](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" id="svg20" preserveAspectRatio="xMidYMid meet" viewBox="0 0 81.000008 61.220618" height="45.348606" width="60.000008" version="1.0">
  <defs id="defs24"/>
  <g style="fill:#000000;stroke:none" id="g18" transform="matrix(0.01202191,0,0,-0.01202191,-1.0847093,61.521167)">
    <path id="path4" d="m 3342,5094 c -171,-50 -259,-116 -324,-241 -38,-72 -94,-231 -85,-240 3,-3 18,19 33,49 46,90 94,160 158,226 77,81 131,120 248,182 103,55 95,61 -30,24 z"/>
    <path id="path6" d="m 4140,5093 c -168,-27 -356,-83 -537,-159 -182,-76 -289,-150 -424,-291 -47,-49 -105,-117 -129,-151 -36,-50 -52,-63 -80,-68 -126,-22 -195,-64 -355,-217 l -126,-119 -149,-28 C 1860,3970 1586,3853 1305,3618 1155,3494 1046,3366 957,3215 882,3088 777,2983 686,2945 l -38,-16 20,28 c 24,35 43,91 51,155 l 6,49 44,-61 c 24,-33 46,-58 49,-56 13,13 32,130 32,199 0,82 -16,135 -50,172 -19,21 -19,22 12,103 17,45 29,84 27,87 -12,11 -124,-26 -193,-63 -153,-83 -271,-194 -329,-312 -52,-106 -43,-222 24,-295 10,-11 28,-52 40,-90 30,-99 63,-158 106,-194 47,-39 131,-65 193,-59 l 46,5 -9,-66 c -11,-83 -3,-457 13,-534 11,-55 10,-62 -26,-160 L 667,1735 519,1584 C 372,1436 370,1433 370,1389 371,1217 476,1011 630,882 764,769 892,709 1008,702 c 39,-2 72,-6 72,-8 0,-2 -16,-42 -35,-88 -64,-155 -43,-248 72,-320 66,-41 150,-70 372,-126 186,-48 302,-64 396,-57 178,13 239,89 252,312 6,110 17,139 98,263 52,82 66,133 58,221 -6,58 -4,64 11,58 9,-4 84,-10 165,-13 l 149,-7 -31,-21 c -87,-59 -105,-159 -54,-302 53,-146 237,-418 345,-508 78,-65 124,-81 228,-81 86,0 95,2 175,42 94,47 113,63 163,147 27,45 66,84 173,171 134,109 139,113 164,99 49,-25 154,-34 405,-34 283,0 328,7 381,62 45,46 63,97 70,192 5,72 3,88 -20,141 -23,55 -25,76 -30,250 -6,216 -4,211 -107,471 -37,96 -66,174 -63,174 2,0 61,-52 130,-115 162,-148 218,-178 349,-183 144,-7 263,37 378,139 69,61 117,140 150,248 l 29,93 41,-6 c 22,-3 95,-9 161,-12 210,-12 408,23 564,97 309,146 510,440 586,856 23,123 30,392 15,554 -22,242 -69,462 -135,635 -43,111 -70,148 -141,196 -65,44 -109,58 -182,58 -86,0 -113,-20 -218,-169 -58,-82 -110,-128 -179,-156 l -34,-15 -41,78 c -46,88 -121,191 -187,257 -25,25 -105,86 -178,135 -73,50 -206,153 -296,231 -261,224 -369,298 -533,365 -131,53 -245,74 -401,73 -71,-1 -141,-4 -155,-6 z m 420,-162 c 164,-48 311,-138 528,-324 67,-58 149,-127 183,-153 64,-51 64,-55 -6,-70 -37,-7 -35,-8 44,-16 66,-6 96,-15 140,-40 96,-53 158,-105 204,-170 61,-87 141,-227 134,-235 -4,-3 -26,2 -50,11 -61,23 -99,20 -175,-13 -37,-16 -87,-35 -112,-43 l -45,-13 51,-3 50,-3 -3,-62 c -4,-60 -3,-61 11,-31 14,32 33,44 67,44 26,0 59,-35 59,-64 0,-21 -9,-27 -72,-48 l -73,-23 137,-3 c 151,-3 140,2 154,-74 7,-44 -6,-49 -58,-22 -23,12 -47,15 -87,11 -139,-14 -223,-66 -259,-160 -60,-160 -71,-191 -69,-194 2,-1 30,45 63,104 67,120 103,154 193,179 31,9 64,18 74,21 10,2 -4,-17 -30,-44 -58,-57 -69,-90 -88,-251 -7,-65 -17,-130 -20,-145 -4,-15 -5,-27 -1,-27 3,0 24,60 47,133 23,72 50,148 59,167 35,69 113,130 167,130 h 31 l 7,-86 c 8,-94 2,-193 -20,-319 -29,-166 -28,-153 -11,-125 24,39 53,135 82,265 29,137 34,267 14,412 -19,141 -6,160 150,218 86,33 128,68 231,195 40,50 83,93 98,98 61,21 136,-30 180,-121 85,-178 151,-551 151,-854 0,-249 -41,-438 -140,-638 -125,-254 -295,-405 -540,-475 -108,-32 -327,-38 -475,-15 -215,33 -524,128 -639,195 -38,22 -70,39 -72,37 -2,-1 1,-32 6,-67 11,-74 7,-109 -26,-239 -32,-125 -32,-225 -1,-272 46,-70 158,-93 247,-52 l 45,21 -52,-5 c -75,-8 -138,4 -173,33 -42,36 -37,46 13,28 61,-23 162,-23 219,-1 56,22 103,72 117,124 6,21 17,63 26,94 l 15,56 40,-10 c 22,-6 40,-16 40,-22 0,-33 -64,-207 -89,-241 -91,-126 -276,-194 -408,-150 -68,23 -113,55 -276,198 -137,121 -147,128 -243,163 -124,47 -177,73 -283,142 l -84,54 31,-38 c 63,-80 158,-156 260,-207 l 52,-26 -6,-43 C 4268,1645 4136,1244 4054,1095 3986,973 3841,753 3809,724 3796,712 3693,630 3580,541 3418,413 3362,364 3315,302 3249,217 3176,170 3111,170 c -54,0 -130,41 -183,98 -101,108 -240,349 -254,438 -11,74 5,96 115,158 53,30 123,73 155,95 l 60,40 29,-40 c 16,-22 31,-39 32,-37 2,2 -2,32 -7,68 -8,58 -4,266 8,330 6,37 -41,-64 -66,-141 l -21,-66 -82,-12 c -184,-29 -469,-30 -627,-3 l -45,7 -25,125 -25,125 -5,-255 c -6,-286 -4,-278 -93,-416 -76,-120 -84,-140 -90,-244 -9,-164 -29,-185 -172,-184 -101,1 -150,9 -345,61 -161,43 -243,81 -270,126 -26,43 -26,53 15,178 33,102 85,333 85,377 0,11 -22,53 -48,94 -158,239 -293,538 -356,794 -40,160 -50,247 -50,449 -1,219 11,293 74,462 109,289 264,521 458,687 260,221 518,338 894,404 l 108,19 -40,-46 c -21,-25 -102,-120 -180,-211 -168,-197 -228,-283 -320,-465 -83,-161 -115,-257 -107,-321 6,-60 54,-152 124,-244 90,-117 158,-224 183,-285 l 23,-55 -26,-78 c -14,-43 -29,-109 -33,-146 l -7,-68 74,-75 c 148,-150 237,-203 374,-224 63,-9 96,-9 155,1 77,13 78,13 430,198 175,91 324,145 452,163 l 87,12 30,96 c 46,143 62,232 68,372 7,148 -6,285 -46,499 -32,170 -57,359 -67,518 -4,56 -10,105 -14,107 -9,6 -26,-85 -41,-220 -15,-129 -6,-282 26,-475 32,-189 43,-421 26,-540 -13,-85 -64,-245 -83,-256 -6,-4 -46,-13 -90,-21 -132,-22 -272,-83 -528,-226 -195,-109 -199,-111 -320,-111 -146,-1 -192,19 -310,131 -49,46 -92,93 -95,103 -3,10 -1,49 5,86 12,82 101,267 238,499 131,222 136,231 125,220 -33,-33 -230,-274 -282,-345 -35,-47 -66,-86 -70,-87 -3,-1 -18,24 -33,55 -15,31 -68,111 -118,177 -104,139 -130,188 -130,249 0,57 15,97 94,255 84,168 120,220 269,389 67,76 192,219 278,318 190,218 368,392 429,422 27,13 109,31 205,47 178,28 417,74 425,82 5,5 -159,-2 -312,-13 -44,-3 -78,-1 -78,4 0,19 161,200 218,245 146,115 489,256 737,303 106,20 340,12 435,-15 z M 651,3379 c -22,-49 -41,-150 -41,-217 0,-69 -20,-128 -56,-171 l -35,-40 31,-60 c 29,-59 53,-81 84,-81 28,0 126,41 175,74 l 48,33 -30,-88 -30,-87 -57,-16 c -70,-20 -120,-20 -165,-2 -43,18 -58,43 -94,149 -16,47 -40,100 -53,117 -18,25 -23,44 -23,93 1,71 27,129 95,209 39,45 159,142 166,135 3,-2 -5,-24 -15,-48 z M 924,1470 c 44,-104 142,-290 200,-383 49,-79 54,-90 45,-119 -23,-84 -64,-118 -140,-118 -123,0 -358,170 -436,317 -31,58 -63,157 -63,196 0,12 54,75 129,151 94,96 132,142 141,171 7,22 15,44 18,49 3,5 22,-38 42,-95 20,-57 49,-133 64,-169 z m 3454,-13 c 60,-162 69,-208 76,-410 5,-144 10,-196 22,-220 12,-23 15,-49 12,-103 -5,-80 -24,-119 -64,-134 -52,-20 -498,-7 -558,16 -13,5 -7,16 31,58 66,73 182,253 250,387 35,68 84,193 123,310 36,108 67,195 70,192 2,-2 20,-45 38,-96 z"/>
    <path id="path8" d="m 4395,4151 c -111,-40 -200,-102 -231,-163 -22,-43 -34,-119 -25,-174 l 7,-49 22,70 c 23,75 80,172 128,217 16,15 60,47 99,72 75,50 75,54 0,27 z"/>
    <path id="path10" d="m 3285,4129 c -238,-26 -230,-24 -291,-81 -65,-61 -126,-174 -184,-343 -44,-130 -93,-295 -88,-301 2,-1 36,72 76,164 107,248 193,385 264,423 15,8 14,2 -7,-43 -20,-43 -25,-68 -24,-133 1,-44 4,-91 8,-105 6,-21 9,-16 20,33 16,73 68,176 125,248 41,52 50,58 118,78 40,12 109,35 153,51 74,26 77,29 40,28 -22,0 -116,-9 -210,-19 z"/>
    <path id="path12" d="m 6411,4074 c -36,-68 -146,-181 -228,-235 -82,-53 -153,-74 -224,-66 -27,3 -49,4 -49,2 0,-12 122,-36 166,-33 138,12 282,139 344,305 27,72 22,86 -9,27 z"/>
    <path id="path14" d="m 4825,3775 c -22,-7 -80,-22 -130,-34 -51,-12 -104,-31 -125,-46 -38,-27 -243,-265 -235,-273 3,-3 31,13 62,34 32,21 61,40 64,42 3,1 12,-17 18,-41 7,-24 23,-56 34,-71 l 22,-27 -3,44 c -3,35 1,50 17,66 41,41 105,36 135,-11 8,-14 -1,-29 -55,-85 -66,-68 -80,-88 -99,-139 -8,-21 7,-10 68,48 122,116 262,205 410,258 34,13 60,25 58,28 -5,4 -108,-12 -152,-24 -17,-5 -22,0 -27,23 -8,40 -47,94 -84,118 -18,11 -32,22 -33,25 0,3 20,18 45,33 35,23 71,53 53,46 -2,-1 -21,-7 -43,-14 z"/>
    <path id="path16" d="m 329,3563 c -133,-97 -196,-178 -225,-288 -16,-60 -19,-335 -4,-335 6,0 10,18 10,41 0,72 40,240 75,317 38,82 98,170 170,250 26,29 45,55 43,58 -3,2 -34,-17 -69,-43 z"/>
  </g>
</svg>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a json dataset using spark\n",
    "If you know the schema or the fields you want to query, you should specify it. Otherwise the spark.read.json will try to infer the schema reading the whole dataset (by default, you can use an option to make it use only a sample). \n",
    "Most of the time you only need a subset of the available fields, you only need to specify that subset in the schema. \n",
    "\n",
    "*Run the following cell to create a spark dataframe with the desired fields.*\n",
    "\n",
    "To see the efects of specifying the schema, you could run the same cell without specifying the schema. **WARNING**: Spark operations are lazy, they execute only when you call an action, but the infer schema step will start inmediatelly and for this data can take around 8 minutes to finish with 64 executors and two cores per executor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "_schema = StructType([\n",
    "    StructField('metadata', StructType([StructField(\"_id\",StringType(), nullable=False), \n",
    "                                        StructField(\"timestamp\",LongType(), nullable=False)])),\n",
    "    StructField('data', StructType([\n",
    "        StructField(\"GlobalJobId\", StringType(), nullable=False),\n",
    "        StructField(\"RecordTime\", LongType(), nullable=False),\n",
    "        StructField(\"Status\", StringType(), nullable=True),\n",
    "        StructField(\"CMSPrimaryPrimaryDataset\", StringType(), nullable=True),\n",
    "        \n",
    "    ])),\n",
    "])\n",
    "\n",
    "condor_df = spark.read.json(\"/project/monitoring/archive/condor/raw/metric/2019/10/31\", schema=_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the data, for example we can query for the most popular datasets (the dataset that appears on more jobs as primary dataset), considering only the completed jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we select the fields we want to work with. (Exploding the nested structures will make easier to work with the data)\n",
    "# We filter the records to only take into account the ones we want. \n",
    "# And we drop duplicates, as we want to count every job only once. \n",
    "condor_df = condor_df.select(\"data.*\").where(\"Status = 'Completed' AND CMSPrimaryPrimaryDataset IS NOT NULL\").drop_duplicates([\"GlobalJobId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we group and count by Dataset, and filter out the datasets that appear on only one job. \n",
    "grouped_data = condor_df.groupby(\"CMSPrimaryPrimaryDataset\").count().where(\"count>1\").orderBy(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, so far, we haven't perform any opperation on our data, because all the methods we have used so far are transformations. \n",
    "\n",
    "Now we will use an action to create the spark job. \n",
    "As we know that we will have small number of rows and only two columns we can use the toPandas method to get the data into the driver as a pandas dataframe. Be aware of the amount of data you get into the driver as it must fit into memory. If your results are two big to collect them in one machine you can write them back to HDFS.\n",
    "```\n",
    "grouped_data.write.csv(\"outputFolder\",header=\"True\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have seen the result on a small time period, try changing the file path to have several days. You can do it either changing the string for a list of paths, or using glob expressions. \n",
    "e.g.\n",
    "\n",
    "```python \n",
    "# all October\n",
    "condor_df = spark.read.json(\"/project/monitoring/archive/condor/raw/metric/2019/10/*\", schema=_schema)\n",
    "\n",
    "```\n",
    "\n",
    "```python \n",
    "# May to October 2019\n",
    "condor_df = spark.read.json(\"/project/monitoring/archive/condor/raw/metric/2019/{05,06,07,08,09,10}/*\", schema=_schema)\n",
    "```\n",
    "```python \n",
    "# May to October 2019 (as list)\n",
    "condor_df = spark.read.json([\"/project/monitoring/archive/condor/raw/metric/2019/05/*\",\n",
    "                             \"/project/monitoring/archive/condor/raw/metric/2019/06/*\",\n",
    "                             \"/project/monitoring/archive/condor/raw/metric/2019/07/*\",\n",
    "                             \"/project/monitoring/archive/condor/raw/metric/2019/08/*\",\n",
    "                             \"/project/monitoring/archive/condor/raw/metric/2019/09/*\",\n",
    "                             \"/project/monitoring/archive/condor/raw/metric/2019/10/*\"], schema=_schema)\n",
    "```\n",
    "(As this parameters include glob expressions, the json will trigger a job to find the leaf nodes, i.e. the actual filenames). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = StructType([\n",
    "    StructField('metadata', StructType([StructField(\"_id\",StringType(), nullable=False), \n",
    "                                        StructField(\"timestamp\",LongType(), nullable=False)])),\n",
    "    StructField('data', StructType([\n",
    "        StructField(\"GlobalJobId\", StringType(), nullable=False),\n",
    "        StructField(\"RecordTime\", LongType(), nullable=False),\n",
    "        StructField(\"Status\", StringType(), nullable=True),\n",
    "        StructField(\"CMSPrimaryPrimaryDataset\", StringType(), nullable=True),\n",
    "        \n",
    "    ])),\n",
    "])\n",
    "\n",
    "condor_df = spark.read.json(\"/project/monitoring/archive/condor/raw/metric/2019/{05,06,07,08,09,10}/*\", schema=_schema)\n",
    "# First, we select the fields we want to work with. (Exploding the nested structures will make easier to work with the data)\n",
    "# We filter the records to only take into account the ones we want. \n",
    "# And we drop duplicates, as we want to count every job only once. \n",
    "condor_df = condor_df.select(\"data.*\").where(\"Status = 'Completed' AND CMSPrimaryPrimaryDataset IS NOT NULL\").drop_duplicates([\"GlobalJobId\"])\n",
    "# Now we group and count by Dataset, and filter out the datasets that appear on only one job. \n",
    "grouped_data = condor_df.groupby(\"CMSPrimaryPrimaryDataset\").count()\\\n",
    "                        .where(\"count>1\")\\\n",
    "                        .orderBy(\"count\", ascending=False)\n",
    "# We only want the top datasets, so we can limit the result before transfering\n",
    "grouped_data.limit(500).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.executor.memory",
     "value": "4g"
    },
    {
     "name": "spark.driver.memory",
     "value": "4g"
    },
    {
     "name": "spark.executor.instances",
     "value": "64"
    },
    {
     "name": "spark.executor.cores",
     "value": "4"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
