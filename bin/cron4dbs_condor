#!/bin/bash
# this script is designed to be used in crontab, i.e. with full path
# adjust script to put your desired notification address if necessary

#addr=cms-popdb-alarms@cern.ch
#addr=vkuznet@gmail.com
addr=cms-comp-monit-alerts@cern.ch

# DO NOT EDIT BELOW THIS LINE
# for Spark 2.X
export PATH=$PATH:/usr/hdp/hadoop/bin
export HADOOP_CONF_DIR=/etc/hadoop/conf

hdir=hdfs:///project/monitoring/archive/condor/raw/metric
currentMonth=$(date +'%Y/%m')
previousMonth=$(date --date='1 month ago' +'%Y/%m')
# List all folders from current and prev month, exclude .tmp folders in grep, sort them, get the latest non tmp folder
lastSnapshot=$(hadoop fs -ls "${hdir}/${currentMonth}"/ "${hdir}/${previousMonth}/" | grep -v .tmp$ | grep $hdir | awk '{print $8}' | sort -nr | head -1)
echo "Last available snapshot: ${lastSnapshot}"

date=$(echo "$lastSnapshot" | sed -e "s,${hdir},,g" -e "s,/,,g")
odir=hdfs:///cms/dbs_condor

# set log area and environment
me=$(dirname "$0")
wdir=$(echo $me | sed -e "s,/bin,,g")
mkdir -p $wdir/logs
log=$wdir/logs/dbs_condor-$(date +%Y%m%d).log
export PYTHONPATH=$wdir/src/python:$PYTHONPATH
export PATH=$wdir/bin:$PATH

echo "Start job for ${date+'%Y/%m/%d'} ..." >>$log 2>&1
hadoop fs -ls ${hdir}/"$currentMonth"/ >>$log 2>&1
echo "Date to run for (lastSnapshot): $date" >>$log 2>&1

# check if we got a date to run
if [ -z "${date}" ]; then
    echo "No date to run" >>$log 2>&1
    exit
fi

dotmp=$(echo $date | grep tmp)
if [ -n "${dotmp}" ]; then
    echo "Date ends with tmp" >>$log 2>&1
    exit
fi

# create appropriate area in our output directory, do not include day dir since it will be created by a job
hadoop fs -mkdir -p ${odir}/campaign/"$currentMonth"
hadoop fs -mkdir -p ${odir}/dataset/"$currentMonth"
hadoop fs -mkdir -p ${odir}/release/"$currentMonth"
hadoop fs -mkdir -p ${odir}/era/"$currentMonth"

# check that we need to run
oday=$(echo $date | cut -c 7-8)
check=$(hadoop fs -ls ${odir}/dataset/"$currentMonth"/${oday} 2>/dev/null)
if [ -n "${check}" ]; then
    echo "No dir to run" >>$log 2>&1
    exit
fi

# setup to run the script
cmd="$wdir/bin/run_spark dbs_condor.py --yarn --fout=$odir --date=$date"
#echo "Will execute ..."
#echo $cmd
msg="Error while executing $cmd on $USER@$(hostname) log at $log"
set -e
#trap "echo \"$msg\" | mail -s \"Cron alert run_spark dbs_condor\" \"$addr\"" ERR
# Call func function on exit
trap func exit
# Declare the function
function func() {
    local status=$?
    if [ $status -ne 0 ]; then
        local msg="cron4dbs_condor completed with non zero status"
        if [ -f /data/cms/bin/amtool ]; then
            local expire=$(date -d '+1 hour' --rfc-3339=ns | tr ' ' 'T')
            local urls="http://cms-monitoring.cern.ch:30093 http://cms-monitoring-ha1.cern.ch:30093 http://cms-monitoring-ha2.cern.ch:30093"
            for url in $urls; do
                /data/cms/bin/amtool alert add cron4dbs_condor \
                    alertname=cron4dbs_condor severity=medium tag=cronjob alert=amtool \
                    --end=$expire \
                    --annotation=summary="$msg" \
                    --annotation=date="$(date)" \
                    --annotation=hostname="$(hostname)" \
                    --annotation=status="$status" \
                    --annotation=command="$cmd" \
                    --annotation=log="$log" \
                    --alertmanager.url=$url
            done
        else
            echo "$msg" | mail -s "Cron alert cron4dbs_condor" "$addr"
        fi
    fi
}

$cmd >> $log 2>&1

# ---------------------------------------------------------------------------------------------------------------- TEST
# This cron job runs daily, so last modification date of the output should be within 12 hours
time_threshold=43200
# Size thresholds
campaign=200000000 # (20MB)
dataset=200000000 # (20MB)
era=100000 # (100KB)
release=100000 # (100KB)

script_dir="$(cd "$(dirname "$0")" && pwd)"
date_hdfs=$(date -d "yesterday" '+%Y/%m/%d')

set -e
/bin/bash  "$script_dir/utils/check_utils.sh" check_hdfs "$odir/campaign/$date_hdfs" $time_threshold $campaign
/bin/bash  "$script_dir/utils/check_utils.sh" check_hdfs "$odir/dataset/$date_hdfs" $time_threshold $dataset
/bin/bash  "$script_dir/utils/check_utils.sh" check_hdfs "$odir/era/$date_hdfs" $time_threshold $era
/bin/bash  "$script_dir/utils/check_utils.sh" check_hdfs "$odir/release/$date_hdfs" $time_threshold $release
# Running commands after this point will change the exit code.
