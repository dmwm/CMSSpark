#!/bin/bash
set -e
##H  Author: Valentin Kuznetsov <vkuznet AT gmail [DOT] com>
##H  Usage:   run_aggregation <configuration> <date>
##H  Example: PYTHONPATH=/path/CMSSpark/src/python run_aggregation conf.json 20170925
##H  Requires conf.json as first argument:
##H    {
##H        "output_dir" : "hdfs:///cms/cms-popularity/agg",
##H        "py_files" : "CMSMonitoring.zip,stomp-v700.zip",
##H        "credentials" : "amq_broker.json",
##H        "keytab" : "keytab",
##H        "aaa_dir" : "/project/monitoring/archive/xrootd/enr/gled",
##H        "cmssw_dir" : "hdfs:///project/monitoring/archive/cmssw_pop/raw/metric",
##H        "eos_dir" : "/project/monitoring/archive/eos/logs/reports/cms",
##H        "jm_dir" : "/project/awg/cms/jm-data-popularity/avro-snappy"
##H    }
##H  py_files: required for spark-submit, see how to create them https://gist.github.com/mrceyhun/3ac99f929f78334e80e15c38870adc49
##H
# TODO modify to run in K8s
script_dir="$(cd "$(dirname "$0")" && pwd)"
# get common util functions
. "$script_dir"/utils/common_utils.sh

trap "echo \"abnormal exit\"" ERR

last_non_temp_short_date() {
    # hadoop fs -ls -R $1 - get list of all files and directories in $1 (recursively)
    # grep -E ".*[0-9]{4}/[0-9]{2}/[0-9]{2}$" - get only lines that end with dddd/dd/dd (d - digit)
    # tail -n1 - get the last line (last directory)
    # tail -c11 - get last 11 characters (+1 for newline)
    # sed -e "s/\///g" - replace all / with nothing (delete /)
    result=$(hadoop fs -ls -R "$1" | grep -E ".*[0-9]{4}/[0-9]{2}/[0-9]{2}$" | tail -n1 | tail -c11 | sed -e "s/\///g")
    echo "$result"
    return 0
}

last_non_temp_long_date() {
    # hadoop fs -ls -R $1 - get list of all files and directories in $1 (recursively)
    # sort -n - sort entries by comparing according to string numerical value
    # grep -E ".*year=[0-9]{4}/month=[0-9]{1,2}/day=[0-9]{1,2}$" - get only lines that end with year=dddd/month=dd/day=dd (d - digit)
    # tail -n1 - get the last line (last directory)
    # cut -d "=" -f 2- - get substring from first =
    # sed -E "s/[a-z]*=[0-9]{1}(\/|$)/0&/g" - replace all word=d (d - digit) with 0word=d
    # sed -E "s/[^0-9]//g" - delete all characters that are not digits
    result=$(hadoop fs -ls -R "$1" | sort -n | grep -E ".*year=[0-9]{4}/month=[0-9]{1,2}/day=[0-9]{1,2}$" | tail -n1 | cut -d "=" -f 2- | sed -E "s/[a-z]*=[0-9]{1}(\/|$)/0&/g" | sed -E "s/[^0-9]//g")
    echo "$result"
    return 0
}

if [ $# -eq 0 ] || [ "$1" == "-h" ] || [ "$1" == "-help" ] || [ "$1" == "--help" ]; then
    util_usage_help
    exit 0
fi

# First argument
conf="$1"

# Setup Analytix cluster settings for Lxplus7 or Kubernetes
if [[ -n "$IS_K8S" ]]; then
    util4logi "Attempted to run in K8s, checking required environment variables"
    util_setup_spark_k8s
    util4logi "running in K8s"
else
    util4logi "Running on Lxplus7"
    util_setup_spark_lxplus7
    conf=()
fi

util4logi "Read configuration: $conf"
export PATH=$script_dir:$PATH
export PYTHONPATH=$script_dir/../src/python:$PYTHONPATH

# parse configuration
output_dir=$(python -c "import sys, json; print(json.load(sys.stdin)['output_dir'])" <"$conf")
py_files=$(python -c "import sys, json; print(json.load(sys.stdin)['py_files'])" <"$conf")
credentials=$(python -c "import sys, json; print(json.load(sys.stdin)['credentials'])" <"$conf")
keytab=$(python -c "import sys, json; print(json.load(sys.stdin)['keytab'])" <"$conf")

aaa_dir=$(python -c "import sys, json; print(json.load(sys.stdin)['aaa_dir'])" <"$conf")
cmssw_dir=$(python -c "import sys, json; print(json.load(sys.stdin)['cmssw_dir'])" <"$conf")
eos_dir=$(python -c "import sys, json; print(json.load(sys.stdin)['eos_dir'])" <"$conf")
jm_dir=$(python -c "import sys, json; print(json.load(sys.stdin)['jm_dir'])" <"$conf")

if [ -z "$aaa_dir" ] || [ -z "$cmssw_dir" ] || [ -z "$eos_dir" ] || [ -z "$jm_dir" ]; then
    util4loge "Empty data-stream input"
    exit 1
fi

util4logi "AAA $aaa_dir"
util4logi "CMSSW $cmssw_dir"
util4logi "EOS $eos_dir"
util4logi "CRAB $jm_dir"

aaa_date=""
eos_date=""
cmssw_date=""
jm_date=""

# Authenticate kerberos and get principle user name
kerberos_user=$(util_kerberos_auth_with_keytab "$keytab")
util4logi "Authenticated kerberos with user $kerberos_user"

if [ $# -eq 2 ]; then
    util4logi "use date $2"
    aaa_date="$2"
    eos_date="$aaa_date"
    cmssw_date="$aaa_date"
    jm_date="$aaa_date"
else
    util4logi "construct date"
    aaa_date=$(last_non_temp_short_date "$aaa_dir")
    eos_date=$(last_non_temp_short_date "$eos_dir")
    cmssw_date=$(last_non_temp_short_date "$cmssw_dir")
    jm_date=$(last_non_temp_long_date "$jm_dir")
    min_date=$(echo "$aaa_date $eos_date $cmssw_date $jm_date" | tr ' ' '\n' | sort | head -1)
    util4logi "AAA date $aaa_date"
    util4logi "CMSSW date $cmssw_date"
    util4logi "EOS date $eos_date"
    util4logi "JM date $jm_date"
    util4logi "minimum date across data streams: $min_date, will use it ..."
    aaa_date=$min_date
    eos_date=$min_date
    cmssw_date=$min_date
    jm_date=$min_date
fi

util4logi "----------------------------------------------"
util4logi "Starting script"

util4logi "AAA date $aaa_date"
util4logi "CMSSW date $cmssw_date"
util4logi "EOS date $eos_date"
util4logi "JM date $jm_date"

if [ -z "$aaa_date" ] || [ -z "$cmssw_date" ] || [ -z "$eos_date" ] || [ -z "$jm_date" ]; then
    util4loge "Unable to find dates"
    exit 1
fi

if [ "$aaa_date" != "" ] && [ "$aaa_date" == "$cmssw_date" ] && [ "$cmssw_date" == "$eos_date" ] && [ "$eos_date" == "$jm_date" ]; then
    util4logi "All streams are ready for $aaa_date"
    output_dir_with_date="$output_dir/${aaa_date:0:4}/${aaa_date:4:2}/${aaa_date:6:2}"
    util4logi "Output directory $output_dir_with_date"
    output_dir_ls=$(hadoop fs -ls "$output_dir_with_date" | tail -n1)
    if [ "$output_dir_ls" != "" ]; then
        util4logi "Output at $output_dir_with_date exist, will cancel"
    else
        util4logi "Output at $output_dir_with_date does not exist, will run"
        # Add --verbose for verbose output
        run_spark3 data_aggregation.py --date "$aaa_date" --fout "$output_dir"
        if [ $? == 1 ]; then
            util4loge "Abnormal exit of run_spark3 data_aggregation.py"
            exit 1
        fi
        run_spark3 cern_monit3.py --hdir "$output_dir_with_date" --amq "$credentials" --verbose --aggregation_schema --py-files="$py_files"
        if [ $? == 1 ]; then
            echo "Abnormal exit of run_spark3 cern_monit.py"
            exit 1
        fi
    fi

else
    util4loge "Not running script because not all streams are ready"
fi

util4logi "Finishing script"
