#!/bin/sh
# Author: Valentin Kuznetsov <vkuznet AT gmail [DOT] com>
# A wrapper script to submit spark job with CMSSpark python script.

# test arguments
if [ "$#" -eq 0 ]; then
    echo "Usage: run_spark <cmsspark_script> <options>"
    echo "Supported scripts: dbs_phedex.py dbs_aaa.py, dbs_eos.py dbs_cmssw.py cern_monit.py"
    exit 1
fi

# find out where CMSSpark is installed on a system
droot=`python -c "import CMSSpark; print '/'.join(CMSSpark.__file__.split('/')[:-1])"`
cmsspark=$droot/$1

# allow to disable spark output, client should setup his/her own log4j.properties
# via WMA_LOG4J environment variable
conf=""
# look if we requested to show full log output
if [[ "$@" =~ "--no-log4j" ]]; then
    conf=" --conf spark.ui.showConsoleProgress=false "
    if [ -n "$LOG4J_CONF" ] && [ -f $LOG4J_CONF ]; then
        conf="$conf --conf spark.driver.extraJavaOptions=\"-Dlog4j.configuration=file:$LOG4J_CONF\""
    fi
fi

# set avro jars
csvjar=/afs/cern.ch/user/l/lmeniche/public/spark-csv-assembly-1.4.0.jar
avrojar=/usr/lib/avro/avro-mapred.jar
sparkexjar=`ls /usr/lib/spark/examples/lib/spark-examples* | tail -1`

args="${@:2}"
echo "$cmsspark $args"
yarn=`echo $args | grep yarn`

if [ "$2" == "-h" ] || [ "$2" == "--help" ] || [ "$2" == "-help" ]; then
    # run help
    python $cmsspark --help
    exit 0
fi
if [ -n "$yarn" ]; then
    # to tune up these numbers:
    #  - executor-memory not more than 5G
    #  - num-executor can be increased (suggested not more than 10)
    #  - cores = 2/4/8
    # Temp solution to have a wrapper for python27 on spark cluster
    # once CERN IT will resolve python version we can remove PYSPARK_PYTHON
    echo "YARN execution"
    PYSPARK_PYTHON='/afs/cern.ch/user/v/valya/public/python27' \
    spark-submit --jars $csvjar,$avrojar,$sparkexjar \
        --master yarn-client \
        --driver-class-path '/usr/lib/hive/lib/*' \
        --driver-java-options '-Dspark.executor.extraClassPath=/usr/lib/hive/lib/*' \
        --executor-memory 5g \
        $conf $cmsspark $args
else
    # submit spark job with our file, please note
    # that user may increase memory options if necessary
    # the executor and driver memory options can be given in human readable form
    # while spark yarn option should use memoryOverhead as KB value.

    # Modify with local[*] to use all the available cores in the node
    #   optionally increase driver memory with --driver-memory 2G (default 1G)
    echo "LOCAL (NO-YARN) execution"
    PYSPARK_PYTHON='/afs/cern.ch/user/v/valya/public/python27' \
    spark-submit --jars $csvjar,$avrojar,$sparkexjar \
        --executor-memory $((`nproc`/4))G \
        --master local[$((`nproc`/4))] \
        --driver-class-path '/usr/lib/hive/lib/*' \
        --driver-java-options '-Dspark.executor.extraClassPath=/usr/lib/hive/lib/*' \
        $conf $cmsspark $args
fi
